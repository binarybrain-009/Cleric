{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_kubectl_command(command):\n",
    "    \"\"\"\n",
    "    Run a kubectl command and return the output.\n",
    "    Args:\n",
    "        command (str): Command to run as a string.\n",
    "    Returns:\n",
    "        str: The command's output or error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run the command\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        \n",
    "        # Check for errors\n",
    "        if result.returncode != 0:\n",
    "            return f\"Error: {result.stderr.strip()}\"\n",
    "        \n",
    "        # Return the command's output\n",
    "        return result.stdout.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Exception occurred: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command Output: minikube\n",
      "type: Control Plane\n",
      "host: Running\n",
      "kubelet: Running\n",
      "apiserver: Running\n",
      "kubeconfig: Configured\n"
     ]
    }
   ],
   "source": [
    "# Example usage: running a kubectl command\n",
    "output = run_kubectl_command(\"minikube status\")\n",
    "print(\"Command Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q pydantic openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class Commands_(BaseModel):\n",
    "    terminal_commands: list[str]\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"terminal_commands\": [\"kubectl get pods --selector=app=my-deployment\"]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_k8s_command(query):\n",
    "  completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "  #   response_format= Commands_,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpfull assistant who analysis the user query and gernates specific kuberneties commands to run to gether relavent information to answer user query. remember you can run 'minikube', 'kubectl' and topics may cover status, info, or logs from Minikube. Do not gernate commands for deleting/editing/chaning. You are only allowed to gernate read commands only. Output in JSON like {\\\"terminal_commands\\\": [\\\"<list of commands>\\\"]}\"},\n",
    "      {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "  )\n",
    "  reasoning = completion.choices[0].message\n",
    "  print(reasoning.content)\n",
    "\n",
    "  return reasoning\n",
    "\n",
    "query = 'Which pod is spawned by my-deployment?'\n",
    "reasoning = get_k8s_command(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^^ Parsing commands ^^\n",
      "^^ Running commands ^^\n",
      "1) kubectl get pods --selector=app=my-deployment\n"
     ]
    }
   ],
   "source": [
    "def run_loop_on_command(reasoning):\n",
    "    result_dict = {}\n",
    "    # If the model refuses to respond, you will get a refusal message\n",
    "    if (reasoning.refusal):\n",
    "        print(reasoning.refusal)\n",
    "    else:\n",
    "        # print(reasoning.content)\n",
    "        # parse into JSON/disctionary\n",
    "        print('^^ Parsing commands ^^')\n",
    "        commands = json.loads(reasoning.content)\n",
    "\n",
    "        # run commands\n",
    "        print('^^ Running commands ^^')\n",
    "        for i, command in enumerate(commands[\"terminal_commands\"]):\n",
    "            print('{})'.format(i+1),command)\n",
    "            result = run_kubectl_command(command)\n",
    "            result_dict[command] = result\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the command output to identify which pod is spawned by \"my-deployment.\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_query_response(query, result_dict):\n",
    "  completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"\"\"You are a helpfull assistant who answer queries about K8s deployed applications while using information from kuberneties commands run results attached below.\n",
    "      Query Scope:\n",
    "       -Only read actions are required.\n",
    "       -Topics may cover status, info, or logs from Minikube.\n",
    "       - Respond with just the answer, omitting identifiers (e.g., \"mongodb\" instead of \"mongodb-56c598c8fc\").\"\"\"},\n",
    "      {\"role\": \"user\", \"content\": \"\"\"user query:{}.\n",
    "       k8s commands results:{}\"\"\".format(query, json.dumps(result_dict))}\n",
    "    ]\n",
    "  )\n",
    "  reasoning = completion.choices[0].message\n",
    "  return reasoning\n",
    "\n",
    "reasoning = get_query_response(query, result_dict)\n",
    "\n",
    "if (reasoning.content):\n",
    "    print(reasoning.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fastapi uvicorn\n",
    "# pip install python-dotenv\n",
    "#  uvicorn main:app --reload --port 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_20240815",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
